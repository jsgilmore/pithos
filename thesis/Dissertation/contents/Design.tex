\chapter{Pithos Design}
\label{chp:DESIGN}

The generic state consistency model was presented in Section \ref{generic_event_update_model}. One of the key challenges that still remain was determined to be designing an authoritative storage module specifically tailored to P2P MMVEs. The root storage model identifies in Section \ref{generic_event_update_model} includes the processes of state management and state persistency for the authoritative objects.

We've identified the main requirements of P2P MMVE state management and persistency in Section \ref{key_challenges_cm} as: scalability, reliability, fairness, responsiveness and security. In Chapter \ref{p2p_MMVE_state_persistency} it is argued that none of the current approaches to state persistency satisfy all identified requirements,

The focus of this thesis is, therefore on state management and persistency in P2P MMVEs. This chapter presents a design that satisfies all the identified requirements, along with implementation details and results.

Pithos, a novel hybrid multi-tiered state persistency architecture is proposed. The novelty of Pithos lies in its support for both a responsive and a fair storage system, while also taking into account security aspects
of distributed storage. There are some storage systems that provide responsive or fair storage, but none that provide both. No storage system, designed specifically for P2P MMVEs, have taken security into account.

If Pithos is incorporated into an existing P2P MMVE consistency architecture, it will add the ability to handle both state management and state persistency. The addition of a robust state persistency mechanism, specifically designed for P2P MMVEs, will bring us one step closer to the creation of a complete P2P MMVE architecture.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Use cases}
\label{use_cases_goals}

The purpose of Pithos is to allow for efficient object storage and retrieval that satisfies all identified requirements. As is evident from authoritative storage in the flow diagram in Figure \ref{fig_event_update_flowdiagram}, Pithos will interface directly with the VE logic, as well as receive updates from the update generator. For the purposes of this discussion, update generation is assumed to be part of game logic.

\begin{figure}[htbp]
 \centering
 \includegraphics[clip=true, viewport=28mm 223mm 82mm 282mm, width=0.3\textwidth]{pithos_use_case}
 \caption{Use case diagram of Pithos}
 \label{fig_pithos_use_case}
\end{figure}

As shown in the use case diagram in Figure \ref{fig_pithos_use_case}, the VE logic should be able to use Pithos in four ways: store, retrieve, modify and remove. These are the use cases generally required of any storage system.

The VE logic will store data when a new object is added to the VE state. This can happen as a consequence of an event leading to the generation of a new object. An example of this is a rocket firing at a target. This event might generate a missile object to be sent towards the target.

Object retrieval will be required every time an event is received. The VE logic will retrieve the object state from memory, which is part of state management. Object states, other than the one being altered, might also be required by the logic to determine the effect an event will have, as discussed in Section \ref{event_logic_update}.

Object modification occurs every time an object update is generated. An object update, by definition, required a modification of the object state. Object removal might also be required to save storage space, although this is not essential to the correct functioning of the storage system.

\section{Designing for the storage requirements}

Pithos is designed to fulfill all use case requirements as well as the storage requirements for P2P MMVE storage architectures as set out in Section \ref{key_challenges_cm}. To achieve both these goals, an architecture was first designed to fulfill all the storage requirements and the use case requirements were then implemented on the designed network.

The inspiration for Pithos come from two observations:
%
\begin{enumerate}
  \item One can combine multiple storage models and arrive at a model which possesses fewer disadvantages than any of the models used.
  \item Responsiveness is greatly increased in a fully distributed model, where there is no intermediate server that relays all information.
\end{enumerate}

Fully distributed architectures are, however, not scalable because the number of messages scaling by $O(N^2)$, where $N$ is the number of nodes in the network.

This section will describe how the design of Pithos satisfies the identified storage requirements. For each of the requirements of responsiveness, reliability, security and fairness, the design decisions made to achieve the specific requirement will discussed. Some design decisions satisfy multiple requirements, therefore different requirements might discuss the same design decision, with a focus on how the decision satisfies the specific requirement.

\subsection{Responsiveness}

In Pithos, responsiveness is achieved by grouping players into fully connected groups, using group-based distance-based storage to distribute objects and using replication to enable the parallel retrieval of objects.

\subsubsection{Group storage}

In a fully connected network, where all nodes are connected to all other nodes, every node is one hop away from every other node. This solution is, however not scalable. In order to achieve a scalable single hop architecture, it was decided to group users in the virtual world. User groups are then fully connected, which allows for highly responsive group interactions. Group sizes are smaller than the overall network size and allows for the fully connected groups to remain scalable.

In virtual worlds, users already group themselves into explicit groups, including questing groups, raid groups and guilds. Implicit groups are also formed by flocking, where multiple users congregate in areas of the virtual environment that are of interest to them \cite{flocking}.

\subsubsection{Distance-based storage}
Grouping users is not sufficient to achieve a responsive system. Users should also require data stored in the group more than data stored outside the group. To this end, distance based storage is used on a group level. This means that objects are stored in the group that is closest to them. The assumption is that users interact more with objects that are closer to them and, therefore, have more interest in closer objects. If a user has interest in a far off object, she will likely move closer to that object.

\subsubsection{Replication}

Multiple copies of an object are stored. Replication improves responsiveness, because multiple copies of an object may be requested in parallel and the object that arrives fastest is used.

\subsection{Reliability}

Reliability is achieved by storing all objects on overlay storage (DHT), in addition to group storage, replicating all stored objects and repairing object replicas as they are removed due to network churn.

\subsubsection{Overlay storage}
Distance-based group storage attempts to maximize the number of requests for objects within a group. The number of actual intra-group vs. inter-group requests will still depend on the grouping algorithm and it might not be possible to ensure that all requests remain in the group.

In order for peers to be able to requests objects stored in other groups, overlay storage is used. Peers belong to both a group and the overlay. Peers can therefore request data from the group and from the overlay. Overlay storage adds reliability by ensuring that out-of-group object request can also be served.

Every retrieve request is requested from both the group and the overlay storage. If any of the storage types fail, the other might still succeed, adding reliability.

\subsubsection{Replication}

Replication allows for multiple peers to leave the network before an object becomes unavailable, improving reliability. Because of network churn, replication is essential to ensuring objects survive network churn. If only a single object is stored, a node unexpectedly leaving the network destroyers an object. There is no chance of an object being recovered when no replicas are used.

\subsubsection{Repair}

Repair further improves reliability by maintaining object replica numbers. When it is discovered that a peer has left the network, peers storing the objects of the peer that left can replicate those objects, thereby maintaining the number of replicas.

A system only using replication's object numbers will steadily decline from the time an object was stored. A system using repair can replace missing replicas, ensuring long term object survivability.

\subsection{Security}

Security cannot exist in a single system layer and has to be ensured on multiple layers, as discussed in Section \ref{characteristics_security}. Some security concepts implemented in Pithos are the use of a certification authority, replication and quorum.

\subsubsection{Certification}

A difference between Pithos and other P2P systems is that it requires all peers to be uniquely identifiable. A main requirement of classic P2P storage architectures is that peers remain anonymous to ensure user privacy. In Pithos, all users are unique identifiable. An object records which user created it and which users modified it. This allows for the identification of users that maliciously alter objects.

\subsubsection{Replication}

Storing multiple replicas attempts to limit the effect that a malicious user might have on the storage system. If a malicious user altered an object, the original object is still stored on multiple peers.

\subsubsection{Quorum}

When security is a concern, multiple parallel retrieval requests can be performed. A quorum mechanism is then used at the receiving peer to compare the different received objects. The object that is returned by most of the peers is then considered the correct object. A peer modifying an object has no effect on the system, since the receiving peer selects the object returned by the other peers.

\subsection{Fairness}

Fairness is achieved by using overlay storage and group storage.

\subsubsection{Group storage}

Objects are uniformly distributed on group peers, ensuring that all group peers share the load of group objects stored.

\subsubsection{Overlay storage}

Overlay storage maps objects and peers to the same uniform key space and stores an object on a peer that is the closest match for the object's key. This uniformly random mapping ensures a uniform distribution of objects in the overlay.


\section{Key modules and mechanisms}

\begin{figure}[htbp]
 \centering
 \includegraphics[clip=true, viewport=7.5cm 2.5cm 26cm 20cm, width=0.7\columnwidth]{CDHT_layout}
 \caption{Layout of the Pithos storage architecture}
 \label{fig_pithos}
\end{figure}
%
Figure \ref{fig_pithos} shows the Pithos architecture. The figure shows groups of fully connected peers (light blue and dark red), where all groups are connected to each other in an P2P overlay through super peers (red).

Pithos groups peers to form a two tiered storage model. The first tier is a storage model at group level (group storage) and the second is a storage model over all groups (overlay storage).

Pithos can be considered as a multi-tiered structured overlay, with group storage an implementation of an $O(1)$ overlay and overlay storage an implementation of an $O(\log (N))$ overlay.

\subsection{Group storage}

If group storage is considered an $O(1)$ structured overlay, it requires all features mentioned in Section \ref{structured_overlay_features}: geometry, routing functionality, a join mechanism, a leave mechanism and bootstrapping.

On a network level, group storage consists out of three node types: super peers, peers and a directory server. It is possible for a single Pithos terminal to be both a peer and a super peer.

\subsubsection{Geometry and routing}

The geometry is fully connected and routing in such a geometry is trivial. A peer's routing table contains a list of all peers in the group. To route a message to a destination peer, the peer address is retrieved from the routing table and a message is directly sent to that peer.

\subsubsection{Super peers and peers}

In group storage, peers handle requests from the higher application layers. Peers also represent users in group storage, which means that peers are the originators of all store, retrieve, modify and remove requests from a group perspective. The peers themselves receive those requests from the higher layer.

Super peers are responsible for representing a group and managing group membership. Super peers also implement object repair.

\subsubsection{Join mechanism and bootstrapping}

The directory server places joining peers into groups represented by super peers. The directory server allows peers to join the network and be placed into groups.

The first task of a Pithos peer is to join the network and a group. This is done using the directory server, which possesses a static IP address and port that is known to every new peer and super peer. Whenever a super peer is created or potentially selected, it advertises its information with the directory server. This information includes its IP address, port and location of the super peer.

When a peer wishes to join the network, it sends its current location to the directory server. The directory server then responds with the address of a super peer, representing an initial group which the peer can join.

The joining peer then sends the same type of join request to the supplied super peer address. The super peer can then elect to accept or reject the joining peer. This extra step is intended to prevent groups from becoming overloaded. The super peer can supply an alternative address for the joining peer to contact. This extra step is also intended to compensate for the movement of super peers. If super peers move and by the time a joining peer contacts a super peer there is another super peer in closer range, the super peer receiving the join request can supply the joining peer with the address of the closer super peer.

If the super peer accepts the joining peer, it sends that peer a list of all peers currently in the group, as well as a list of objects currently available in the group. The super peer also informs all other group peers of the newly joined peer.

\subsubsection{Leave mechanism}

If a peer leaves the network unexpectedly, it will not have the opportunity to inform its group. With no mechanism to detect a peer unexpectedly leaving, group inconsistency will occur. The peer that left might be selected to store an object or might be required to provide an object from storage. These requests will all fail.

In Pithos, two methods were implemented to ensure group consistency and handle peers leaving the group. The first method uses periodic keep-alive messages. At regular intervals, each group peer uniformly randomly selects another peer in the group to send a keep-alive message to. If a peer received a keep-alive message, it responds with an acknowledge message. A peer that does not respond with an acknowledge message within a sufficiently large amount of time is considered to have left the network.

The originating peer of the keep alive message informs the super peer that the target of the keep-alive message is thought to have left the group. The super peer then also sends a keep-alive message to the target peer, to ensure that there isn't some communications issue between the two group peers and the target peer has actually left the network. If the target peer does not respond to the super peer, the super peer informs all group peers that the target peer has left the group.

The super peer verifies that the peer has left, because by design the super peer is selected to be the most available peer in the group on the network. The super peer does not transmit all keep-alive messages in order to prevent overloading of the super peer.

It is accepted that not all group peers might receive keep-alive messages every round, because of the random selection mechanisms, but this is not required. Only that in a sufficiently few number of keep-alive intervals, the peer is found to have left.

The second mechanism uses requests to identify peers that have left the group. Whenever a request is sent to another peer, a timeout is also started. If a peer does not respond to a request within a given amount of time, the same leave mechanism will be activated has the target peer not responded to a keep-alive message.

It should be noted that either of these methods are sufficient to ensure group consistency over some time period, but using both minimises the time it takes to detect a peer leaving the group.

For graceful group leaves, the same leave mechanisms as used in group migration is employed.

\subsubsection{Migration}

Users will constantly being moving in the virtual environment, leading to changes in position. These position changes may lead to users moving from one group to another. It is imperative that all objects stored in Pithos remain available even with users traversing groups.

As users move around in the virtual world, Pithos queries the directory server with the new positions, using the same mechanism described for joining a group in Section \ref{network_join_implementation}. The peer then receives a new super peer address from the directory server, if this address is different from the address of the currently known super peer. When a new address is received, Pithos initiates group migration.

Group migration involves informing the group that the peer is leaving, clearing all statistics and initiating the group join mechanism. Informing the group that the peer is leaving ensures that this peer will not be picked for any storage or retrieval requests, while the group is unaware that the peer has left. If this is not done, the group will only know that a peer has left after a timeout for a request to the peer expires, which reduces request success rates.

\subsubsection{Ledgers and object stores}

The previous sections discussing groups storage has focussed on group storage, purely as an $O(1)$ structured overlay. To enable the storage of objects within the group, an objects store is also required, as well as a way to track which objects are stored on which peers and conversely, which peers contain which objects.

Each peer in the group contains a local authoritative object store, containing all objects stored on the peer by group storage.

Each peer and super peer in the network contain a group ledger to keep track of which objects are stored on which peers. The group ledger is required whenever an object is retrieved within the group to know from which peer an object can be requested. The ledger is also used by the super peer, to determine which objects should be repaired.

\subsubsection{Grouping}

In order for Pithos to build group, a clustering mechanism is required. Two approaches are being evaluated: distributed clustering techniques (for example affinity propagation \cite{affinity_propagation}) and dynamic regioning techniques (for example self-organising spatial publish subscribe (SOSPS) \cite{self_organising_sps_post}).

\emph{Distributed peer clustering techniques}: make use of the flocking behaviour of players to dynamically group players into flocks or clusters \cite{flocking}. The main idea of flocking is that players move around in groups, rather than randomly on their own. It is desirable that user density within groups should remain constant, because a fully distributed architecture is not scalable. This means that groups should merge or split as the user density within them change.

Affinity propagation clusters nodes using a similarity matrix to find similar nodes. The similarity matrix may contain user positions. In this case, affinity propagation will group nodes depending on their location in a virtual world. This algorithm is ideally suited to P2P applications, since it is a distributed clustering algorithm based on message passing.

\emph{Dynamic regioning}: divides the virtual world into regions that can be resized or further divided to maintain constant player densities across regions. SOSPS creates dynamic regions based on a Voronoi overlay network \cite{voronoi_diagrams_survey}. Near constant user density is achieved by increasing and decreasing the area sizes. This system is based on VON, a distributed Voronoi overlay network designed for MMVEs \cite{VON_VAST}.

\subsection{Overlay storage (DHT)}

Overlay storage is generally slower than group storage, since it is implemented by an $O(\log (N))$ overlay, as described in Section \ref{overlay_storage}.

\subsection{Certification}
In order to design a secure distributed storage system, one requirement for the P2P overlay is that nodes should not be able to select their own IDs or it will not be possible to secure the system against attack. Node IDs should rather be assigned securely by some certification authority \cite{secure_overlay_routing}.

To meet this requirement, Pithos implements its own certification authority to assign node IDs securely and promote security in the P2P overlay. A certification server exists that handle ID requests from nodes. The server assigns IDs to nodes and provides the node with a signed certificate that it may use to store data.

Whenever an object is stored or updated in the storage network, nodes have to sign the object to enable the tracking of object changes throughout the life of the object. This system is very different from classic distributed file storage designs that advocate anonymity in storage. The fact that all changes can be tracked to a specific node will simplify the task of eliminating user cheating.

\subsection{Quorum}
A security mechanism that performs object verification was also implemented. When this mechanism is activated, the objects received from multiple retrieve responses are compared to ensure object correctness.

Object verification is an attempt to make Pithos more resistant to malicious peers that attempt to alter data in a way that is not consistent with the environmental logic. When multiple retrieve responses are received, all objects are compared and the object that occurs the most from all the responses is sent to the higher layer.

To test this, the idea of malicious nodes was added to Pithos. When a node starts, there is some percentage change that it will be designated as a malicious node. Whenever a malicious node receives a retrieve request, it retrieves the requested object, but alters it in some random way. This altered object is then sent to the requesting peer. A flag saying that the object has been altered is also sent, for statistics gathering.

\subsection{Replication}
When storing objects in Pithos, replication is used to increase object availability under network churn and for security in the presence of malicious nodes \cite{storage_and_chaching_PAST}. For every object that is stored in Pithos, $k$ object replicas are also stored. The number of replicas ($k$) depends on the degree of network churn as well as the number of expected malicious users in the network. If the network churn is high, more replicas are required to avoid the situation where all $k$ peers hosting an object leaves the network before any object migration can be done.

If a node leaves the network and stops to transmit ``keep alive'' messages, the migration mechanism will detect this and replicate the file on another node. Replication exists intra- as well as inter-group and is useful in ensuring that if a nodes leaves the network, the data are not lost. All object requests are routed to the peer with the next closest ID if the root peer leaves, because of how overly routing functions. The new destination peers will possess the stored files, since Pithos stores overlay replicas at overlay neighbours.

Another reason to replicate game objects is to make the system more secure. If it is known that a certain percentage of users are malicious, it is advantages to have more replicas than malicious users. This will allow for a secure system where object hashes can be compared to determine which nodes are malicious and what version of an object is accurate.

\subsection{Repair}

When objects are stored, as described in Section \ref{store_implementation}, multiple replicas of the same object is stored within a group. This form of redundancy extends the lifetime of an object, but with the presence of constant network churn, all objects will eventually cease to exist.

The solution is to use a repair mechanism to ensure that missing object replicas are constantly replaced. Two types of repair mechanisms were implemented: periodic repair and lazy repair.

With periodic repair, the super peer periodically checks the number of available replicas of every object in its group ledger. If an object contains less than the required number of replicas and there are nodes in the group that do not already store the object, that object is replicated. Because the super peer itself contains no objects, it requests that an object be replicas from a peer that does contain the object. That peer will receive a replicas request for a specific object, select a group peer in a uniformly random fashion that does not already contain the object and initiate a store request to that peer.

Object repair is not loss-less. A peer can be selected to store a new object replica and that peer can leave the group before it stores the object. Because many object replicas exist and the loss of a single object will, therefore, not endanger the life of an object, the extra bandwidth and maintenance requirements could not be justified.

Lossy repair does create the issue that sometime there are more than one object replicas missing. This is solved by having the super peer request multiple object repairs when more than a single object are missing.

Lazy repair attempts to save on bandwidth and super peer load, by only repairing objects when a node leaves the network. With lazy repair, every peer that is informed that a node is leaving the network, removes the peer reference of all objects contained by that peer in the group ledger. When the super peer removes objects, because a peer left the network, it immediately initiates repair requests for all objects contained on the leaving peer. Because repair is lossy, the super peer initiates more than one object repair for a specific object, if more than one object replicas are missing.

\subsection{Distance-based storage}
For Pithos to succeed as an MMVE storage architecture, intra-group data requests should be preferred to inter-group data requests. This requirement, combined with the fact that the grouping algorithm geographically groups players in the virtual world, lends Pithos to a storage system based on distance-based storage. Similar to interest management, the assumption is that players have a limited area of interest and require interaction with a limited number of objects within range.

Therefore, distance-based storage is implemented on a group level rather than an individual level. This means that objects are stored on the nearest group of players, rather than the nearest user. It is assumed that such an approach will alleviate the security and reliability challenges present in distance-based storage \cite{gilmore_p2p_mmog_state_persistency}.

With group-based distance-based storage, it is assumed that because peers now store objects closest to the group, the objects that they are interested in will most likely be stored within their own group. Therefore, most data requests should be intra-group requests. The overlay storage component ensures that nodes that require data, which are not stored within their group, are still able to access requested data.

\section{Satisfying the use cases}
\subsection{Store}
    Store requests are received from the higher level PithosTestApp, described in Section \ref{pithostestapp}. Figure \ref{fig_pithos_msg_flow_store} shows the message flow between all Pithos modules for an object store request.

\begin{figure}[htbp]
 \centering
 \includegraphics[clip=true, viewport=2mm 143mm 185mm 289mm, width=\columnwidth]{Pithos_msg_flows_store}
 \caption{Pithos message flow as a response to a store request}
 \label{fig_pithos_msg_flow_store}
\end{figure}

Within the same node, the PithosTestApp sends a remote procedure call (RPC) to the Pithos communicator module, containing the object to be stored. The communicator forwards all requests from PithosTestApp to the peer logic module. The peer logic module creates a store request message which it sends to both the DHT storage and group storage modules. This is the request from peer logic to the two modules to store the object in both the DHT and group storage. The DHT storage module translates the storage request message into another RPC, which is understood by the DHT module. The internal RPC call is sent to the DHT module via the communicator. The DHT module further handles overlay storage and returns an internal RPC response indicating success or failure. The RPC response is sent via DHT storage, where statistics are gathered, to the peer logic module.

The group storage module receives the write request. The group storage module reads the required number of replicas from a configuration parameter. The group storage module uses its group ledger to select the required number of nodes. It selects nodes in a uniformly random fashion, making sure to never select the same node more than once. If a node is selected more than once, it reduced the effective number of replicas in the network, which reduces the expected object lifetime, as shown when expected object lifetime is modelled in Section \ref{results}.

A storage request is sent to every selected node, containing the object. The requests will arrive at the communicators of the destination nodes and be sent up to the group storage modules. The group storage module will store the object in its object store. The group storage module will acknowledge the reception of the object by sending a response message to the source node. The response message will travel via the communicator to the group storage module, where the module will log failure and success statistics. Every received response is then forwarded to the peer logic module. The peer logic module will monitor all responses and record successes or failures. When a sufficient number of success or failures have been recorded, peer logic informs the PithosTestApp module of either a successful or failed store.

The question arises as to what constitutes a sufficient number of messages. Two types of storage mechanisms have been implemented in Pithos: ``safe'' and ``fast''. Safe storage waits for all responses to be received at the peer logic module. Only after all responses are received is a decision made whether the store was successful or not. If more than half of the group stores as well as the overlay store was successful is it considered a successful store. The PithosTestApp is informed of the status of the store request at this time. Fast storage has the peer logic module wait for the first successful response, before informing the PithosTestApp of a success. If all responses were failures, it informs PithosTestApp of a failure.

Because safe storage ensures that most of the replications were successfully stored, there is a smaller chance of it reporting a false positive than for fast storage. The disadvantage is that from the point of view of PithosTestApp, and by extension any system that uses Pithos, safe puts take a lot longer to perform. Fast storage on the other hand is no less reliable than safe storage, but the chances of a false positive being reported to the higher layer is greater. The advantage of fast storage is that the from the perspective of the module using Pithos, it is performed faster than safe storage. The higher layer can, therefore, start using the stored object faster.
\subsection{Retrieve}
   The process of retrieving data is very similar to that of storing it, as shown in Figure \ref{fig_pithos_msg_flow_retrieve}. The retrieve request, received from the PithosTestApp, contains the hash of the object name which the PithosTestApp requires. This 128 bit hash is sent to the peer logic. The peer logic then requests that object from both DHT storage and group storage. DHT storage requests the object from the DHT module, using an internal RPC request.

\begin{figure}[htbp]
 \centering
 \includegraphics[clip=true, viewport=3mm 73mm 189mm 219mm, width=\columnwidth]{Pithos_msg_flows_retrieve}
 \caption{Pithos message flow as a response to a retrieve request}
 \label{fig_pithos_msg_flow_retrieve}
\end{figure}

    Group storage first checks whether the object that was requested exists within the group by checking its group ledger. This is required, because the PithosTestApp might require an object that is stored in another group. The out of group request will then only fulfilled by the DHT storage module. If the object is housed in the group storage's local store, the object is sent up to the peer logic module. If the object is found in the group ledger, but not on the peer where the requests originated from, a peer has to be selected to retrieve the object from. The number of peers selected depends on a parameter in the configuration file. More requests sent in parallel will improve the probability of the request succeeding as well as reduce the time it takes to serve the request.

    Retrieve requests mainly fail because of nodes leaving a group. If a node leaves before it can serve an object, the object request fails. If multiple requests are sent, it improves the chances of contacting a node that is not about to leave, which improves the probability of retrieval success. More requests also improve lookup times, since some peers might be geographically closer to the requesting peer, thereby possessing a lower latency. Sending multiple requests improves the chances of contacting a peer that is closer, which improves the responsiveness of the systems. The disadvantage of multiple requests is the extra bandwidth required, both in sending the requests and in the duplicate objects that will be returned. The effect of multiple requests on system reliability, responsiveness and overhead will be evaluated in Chapter \ref{chp:EVALUATION}.

    A request is delivered to the destination peer, which forwards the request to the group storage module. The group storage module retrieves the object from its local storage, attaches it to a response message and sends that response to the source peer. The response is forwarded to the group storage module, which collects statistics before passing it on to the peer logic module. After the peer logic module has received a sufficient number of responses, it informs the higher layer of either a successful or failed retrieval. The object itself is attached to a successful request response.

    Again, two types of retrieval have been implemented: ``fast'' and ``safe''. A fast retrieval sends the first successful response, and therefore, the first successful object, to the higher layer. It replies with failure if all requests failed. The safe retrieval request is more resistent to malicious nodes. For safe retrieval, the peer logic waits for a specified number of responses. It then compares all objects received from successful responses and sends an object to the higher layer that had the most matches when compared. This is to counter a malicious peer that might have altered some of the objects before sending them. The reliability benefit with this scheme in the presence of malicious nodes will be shown in Section \ref{reliability_compare_retrieval}.
\subsection{Modify}
   Modifying data is similar to storing data. A request to modify a specific object is received from the PithosTestApp. The request specifies the object ID, the parameter that has to be modified and the new value of the parameter. The update is delivered to the peer logic, where the modify request is sent to DHT and group storage. The Oversim DHT handles the modify request and responds with success or failure. Group storage received the modify request and checks the group ledger whether the object exists within the group. If the object does not exist in the group, it is only modified in the overlay. To keep track of modified objects, each object has a version number that is incremented every time an object is modified. This allows retrieve requests to select the object with the latest version number to be sent to the higher layer.

    If the object exists within the group, all peers that contain the object are identified from the group ledger and modify requests are sent to them. The modify request will be sent to the destination group storage module where the object in the local store is updated and its version number incremented. If a retrieve request receives objects with multiple version numbers, it selects the set of objects with the latest version number for ``fast'' or ``safe'' comparison.
\subsection{Remove}


\section{Conclusion}

This section describes the overarching design principles of Pithos in terms of the goals set out in previous chapters. The Oversim network simulation framework is then described, along with a description of its extension, enabling it to support the generic consistency architecture. The Pithos modules implemented in Oversim are introduced, followed by explanations of the key Pithos mechanisms. All Pithos mechanisms are described with reference to the Oversim modules where they are housed.

The key mechanisms section regularly described multiple methods of mechanism implementation. Some methods are theoretically better than their alternatives or provide improvements under certain conditions.
